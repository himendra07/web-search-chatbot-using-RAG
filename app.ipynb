{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49aceb-c042-409a-8db6-35b81374bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import asyncio\n",
    "import traceback\n",
    "from extract_queries import extract_queries\n",
    "from web_scraper import fetch_web_pages\n",
    "from db_operations import get_embedding_function\n",
    "from prompt_generator import generate_prompt\n",
    "from config import MODEL_NAME\n",
    "\n",
    "# langchain_ollama chat wrapper (may raise if ollama client not installed)\n",
    "# using try/except so the app still runs and shows sensible errors\n",
    "try:\n",
    "    from langchain_ollama.chat_models import ChatOllama\n",
    "    import ollama\n",
    "except Exception:\n",
    "    ChatOllama = None\n",
    "    ollama = None\n",
    "\n",
    "st.set_page_config(page_title=\"RAGify\", page_icon=\"ðŸ¤–\")\n",
    "st.title(\"RAGify â€” Robust\")\n",
    "\n",
    "# Helper: get safe model list (fallback to config.MODEL_NAME)\n",
    "def safe_ollama_models():\n",
    "    try:\n",
    "        if ollama is None:\n",
    "            return [MODEL_NAME]\n",
    "        models = [m.model for m in ollama.list().models if getattr(m, \"model\", None)]\n",
    "        # filter out embedding-only model name if present\n",
    "        models = [m for m in models if \"nomic-embed-text\" not in m]\n",
    "        if not models:\n",
    "            return [MODEL_NAME]\n",
    "        return models\n",
    "    except Exception as e:\n",
    "        # don't crash the app if Ollama list fails\n",
    "        print(\"Warning: could not list ollama models:\", e)\n",
    "        return [MODEL_NAME]\n",
    "\n",
    "# Sidebar controls\n",
    "with st.sidebar:\n",
    "    model_options = safe_ollama_models()\n",
    "    llm_model = st.selectbox(label=\"Select llm model\", options=model_options, index=0)\n",
    "    search_engine = st.selectbox(label=\"Select search engine\", options=[\"duckduckgo\", \"google\"])\n",
    "    n_results = st.number_input(label=\"Select number of web results\", min_value=1, max_value=8, value=4)\n",
    "\n",
    "# Ensure we always have a valid model string\n",
    "if not llm_model:\n",
    "    llm_model = MODEL_NAME\n",
    "llm_model = str(llm_model)\n",
    "\n",
    "# session messages\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = [\n",
    "        {\"role\": \"assistant\", \"content\": \"Hi, I'm a chatbot who can search the web. How can I help you?\"}\n",
    "    ]\n",
    "\n",
    "for msg in st.session_state.messages:\n",
    "    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
    "\n",
    "# generator for streaming LLM output chunks (keeps same shape as original)\n",
    "def chunk_generator(llm, query):\n",
    "    for chunk in llm.stream(query):\n",
    "        yield chunk\n",
    "\n",
    "# Small helper to run async coroutines from Streamlit safely\n",
    "def run_async(coro):\n",
    "    \"\"\"\n",
    "    Try to run an async coroutine; Streamlit runs in a normal Python process\n",
    "    so asyncio.run is usually fine. We attempt asyncio.run and fallback if needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        # fallback for environments with running loop\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# Main chat input handling\n",
    "if usr_msg := st.chat_input(\"Ask me anything (I will search the web)\"):\n",
    "    # append user message to session\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": usr_msg})\n",
    "    st.chat_message(\"user\").write(usr_msg)\n",
    "\n",
    "    # assistant processing\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        try:\n",
    "            with st.spinner(\"Extracting queries...\"):\n",
    "                # defensive: extract_queries should always return a list\n",
    "                try:\n",
    "                    queries = extract_queries(usr_msg, model=llm_model)\n",
    "                except TypeError:\n",
    "                    # fallback in case extract_queries signature differs\n",
    "                    queries = extract_queries(usr_msg)\n",
    "                if not queries or not isinstance(queries, list):\n",
    "                    queries = [usr_msg]\n",
    "                st.write(f\"Search queries: {queries}\")\n",
    "\n",
    "            with st.spinner(\"Searching on the web...\"):\n",
    "                # run web scraping (async) and ignore failures (function is defensive)\n",
    "                try:\n",
    "                    run_async(fetch_web_pages(queries, n_results, provider=search_engine))\n",
    "                except Exception as e:\n",
    "                    # log and continue â€” generate_prompt will handle missing docs\n",
    "                    st.warning(\"Web fetching failed (continuing): \" + str(e))\n",
    "                    print(\"fetch_web_pages error:\", traceback.format_exc())\n",
    "\n",
    "                # get embedding function (may raise if embedding model missing)\n",
    "                try:\n",
    "                    embedding_function = get_embedding_function()\n",
    "                except Exception as e:\n",
    "                    st.error(\"Embedding function failed: \" + str(e))\n",
    "                    # try to continue with a None placeholder (generate_prompt should handle it)\n",
    "                    embedding_function = None\n",
    "\n",
    "            with st.spinner(\"Extracting info from webpages...\"):\n",
    "                try:\n",
    "                    prompt, sources = generate_prompt(usr_msg, embedding_function)\n",
    "                except Exception as e:\n",
    "                    # If prompt generation fails, show helpful message and continue with fallback\n",
    "                    st.warning(\"Could not generate prompt using downloaded pages. Falling back to direct question.\")\n",
    "                    print(\"generate_prompt error:\", traceback.format_exc())\n",
    "                    prompt = usr_msg\n",
    "                    sources = \"[]\"\n",
    "\n",
    "            with st.spinner(\"Generating response...\"):\n",
    "                # ensure ChatOllama is available\n",
    "                if ChatOllama is None:\n",
    "                    st.error(\"ChatOllama client not available. Make sure 'langchain_ollama' is installed and 'ollama' package is importable.\")\n",
    "                    # show prompt and sources so user can debug\n",
    "                    st.write(\"Prompt sent to model (preview):\")\n",
    "                    st.code(prompt if isinstance(prompt, str) else str(prompt)[:1000])\n",
    "                    st.write(\"Sources:\", sources)\n",
    "                else:\n",
    "                    try:\n",
    "                        llm = ChatOllama(model=llm_model, stream=True)\n",
    "                        stream_data = chunk_generator(llm, prompt)\n",
    "                        # stream to the UI\n",
    "                        st.write_stream(stream_data)\n",
    "                        st.write(sources)\n",
    "                    except Exception as e:\n",
    "                        # show the error and fallback explanation\n",
    "                        st.error(\"LLM generation failed: \" + str(e))\n",
    "                        print(\"ChatOllama error:\", traceback.format_exc())\n",
    "                        st.write(\"Model attempted:\", llm_model)\n",
    "                        st.write(\"Prompt preview:\", prompt[:1000] if isinstance(prompt, str) else str(prompt))\n",
    "        except Exception as outer:\n",
    "            st.error(\"Unexpected error: \" + str(outer))\n",
    "            print(\"Unexpected error in assistant flow:\", traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66b4ae7-03a5-4d3e-a12c-5e071d6dd107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in c:\\ldplayer\\new folder\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\ldplayer\\new folder\\lib\\site-packages (from sentence_transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\ldplayer\\new folder\\lib\\site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\ldplayer\\new folder\\lib\\site-packages (from sentence_transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\ldplayer\\new folder\\lib\\site-packages (from sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\ldplayer\\new folder\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\ldplayer\\new folder\\lib\\site-packages (from sentence_transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\ldplayer\\new folder\\lib\\site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\ldplayer\\new folder\\lib\\site-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\ldplayer\\new folder\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\ldplayer\\new folder\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\ldplayer\\new folder\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\ldplayer\\new folder\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\ldplayer\\new folder\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\ldplayer\\new folder\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\ldplayer\\new folder\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\ldplayer\\new folder\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\ldplayer\\new folder\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\ldplayer\\new folder\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\ldplayer\\new folder\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\ldplayer\\new folder\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\ldplayer\\new folder\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\ldplayer\\new folder\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\ldplayer\\new folder\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\ldplayer\\new folder\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\ldplayer\\new folder\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\ldplayer\\new folder\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\ldplayer\\new folder\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\ldplayer\\new folder\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\ldplayer\\new folder\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\ldplayer\\new folder\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
